{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def crawl_1111(dire_para,rang,sect):\n",
    "    j_dict={}\n",
    "    err_ls=[]\n",
    "    j_err_dict={}\n",
    "    logtext=''\n",
    "    date_crawl=datetime.today().strftime(\"%y%m%d\")\n",
    "    print datetime.today().strftime(\"%Y/%m/%d\")+' start: '+datetime.now().strftime(\"%H:%M:%S\")\n",
    "    #1.1 從不同參數組合中抓取職缺列表(網址)\n",
    "    target_wd=wdid[rang[0]:rang[1]]\n",
    "    for p_j in jid:\n",
    "        for p_wd in target_wd:\n",
    "\n",
    "            #需要先抓一次以找到最高頁數\n",
    "            get=0;times=0;max_pg=1;\n",
    "            while get<1:\n",
    "                try:\n",
    "                    soup=getSoup('https://www.1111.com.tw/job-bank/job-index.asp?si=1&wc='+p_wd+'&d0='+p_j+'&fs=1&ps=100&page=1')\n",
    "                    max_pg=int(soup.find('input','keypage')['data-tot'])\n",
    "                    get+=1\n",
    "                except requests.ConnectionError:\n",
    "                    time.sleep(2+randint(1, 9)*0.1)\n",
    "                    times+=1\n",
    "                    print datetime.now().strftime(\"%H:%M:%S\")+' :try '+str(times)+' times due to ConnectionError.'\n",
    "                except:\n",
    "                    time.sleep(2+randint(1, 9)*0.1)\n",
    "                    times+=1\n",
    "                    print datetime.now().strftime(\"%H:%M:%S\")+' :try '+str(times)+' times due to other kinds of error.'\n",
    "\n",
    "            for p_pg in range(1,(max_pg+1)):\n",
    "                try:\n",
    "                    url='https://www.1111.com.tw/job-bank/job-index.asp?si=1&wc='+p_wd+'&d0='+p_j+'&fs=1&ps=100&page='+str(p_pg)\n",
    "                    soup=getSoup(url)\n",
    "                except requests.ConnectionError:  \n",
    "                    err_ls.append(url)\n",
    "                    print datetime.now().strftime(\"%H:%M:%S\")+' :ConnectionError Occured when retrieving job list'\n",
    "                    time.sleep(4+randint(1, 9)*0.1)\n",
    "                except:\n",
    "                    err_ls.append(url)\n",
    "                    print datetime.now().strftime(\"%H:%M:%S\")+' : Other Error Occured when retrieving job list'\n",
    "                    time.sleep(4+randint(1, 9)*0.1) \n",
    "\n",
    "                #從每一頁中存出職缺網址, 公司名稱, 職務名稱\n",
    "                for item in soup.find_all('li','digest'):\n",
    "                    j_dict['http:'+item.find('h3').find('a')['href']]=[item.find('span',itemprop='name').text,\n",
    "                                                                       item.find('h3').find('a').text]\n",
    "        print datetime.now().strftime(\"%H:%M:%S\")+' :Category '+str(p_j)+' finished.'\n",
    "    \n",
    "    text='1-1: '+str(len(err_ls))+' page of list failed to crawl'\n",
    "    print text;logtext+=text+'\\r\\n'\n",
    "    \n",
    "    ##1.2 爬失敗的頁數\n",
    "    for url in err_ls:\n",
    "        get=0;times=0\n",
    "        while get<1:\n",
    "            try:\n",
    "                soup=getSoup(url)\n",
    "                get+=1\n",
    "            except:\n",
    "                time.sleep(1+randint(1, 9)*0.1)\n",
    "                times+=1\n",
    "                print str(datetime.today())+' :try '+str(times)+' times while crawl '+url\n",
    "\n",
    "        for item in soup.find_all('li','digest'):\n",
    "            j_dict['http:'+item.find('h3').find('a')['href']]=[item.find('span',itemprop='name').text,\n",
    "                                                                        item.find('h3').find('a').text]\n",
    "    print '1-2 '+datetime.now().strftime(\"%H:%M:%S\")+' '+str(len(err_ls))+' page of list saved'\n",
    "    ##\n",
    "    \n",
    "    #2.1 存檔xml\n",
    "    root=ET.Element('ls')\n",
    "    root.set('Date',date_crawl)\n",
    "    root.set('ITEM_COUNT','0')\n",
    "\n",
    "    text=str(len(j_dict))+' URL were found. '+datetime.now().strftime(\"%H:%M:%S\")\n",
    "    logtext+=text+'\\r\\n';print text\n",
    "    \n",
    "    for key,value in j_dict.items():\n",
    "        try:\n",
    "            soup=getSoup(key)\n",
    "\n",
    "            ic=int(root.get('ITEM_COUNT'))\n",
    "            root.append(xml_element_generate(soup,key,value))\n",
    "            root.set('ITEM_COUNT',str(ic+1))\n",
    "\n",
    "            if ic%100==0:\n",
    "                time.sleep(1)\n",
    "            if ic%1000==0:\n",
    "                text=datetime.now().strftime(\"%H:%M:%S\")+' : '+str(ic)+'/'+str(len(j_dict))+' jobs downloaded'\n",
    "                print text;logtext+=text+'\\r\\n'\n",
    "        except requests.ConnectionError:  \n",
    "            j_err_dict[key]=[value[0],value[1]]\n",
    "            text=datetime.now().strftime(\"%H:%M:%S\")+' :ConnectionError occured when saving job '+str(key)\n",
    "            print text;logtext+=text+'\\r\\n'\n",
    "            time.sleep(2+randint(1, 9)*0.1)\n",
    "        except:\n",
    "            j_err_dict[key]=[value[0],value[1]]\n",
    "            text=datetime.now().strftime(\"%H:%M:%S\")+' : Other types of error occurred when saving job '+str(key)\n",
    "            print text;logtext+=text+'\\r\\n'\n",
    "            time.sleep(2)\n",
    "\n",
    "    text='2-1: '+datetime.now().strftime(\"%H:%M:%S\")+' '+root.get('ITEM_COUNT')+' jobs data were crawled.  '\n",
    "    print text;logtext+='\\r\\n\\r\\n'+text+'\\r\\n'\n",
    "    text=str(len(j_err_dict))+' jobs faied to crawled in 2-1'\n",
    "    print text;logtext+=text+'\\r\\n'\n",
    "    \n",
    "    #2.2 重新抓2.1發生錯誤的部分\n",
    "    i=0\n",
    "    for key,value in j_err_dict.items():\n",
    "        get=0\n",
    "        while get<1:\n",
    "            try:\n",
    "                soup=getSoup(key)\n",
    "\n",
    "                ic=int(root.get('ITEM_COUNT'))\n",
    "                root.append(xml_element_generate(soup,key,value))\n",
    "                root.set('ITEM_COUNT',str(ic+1))\n",
    "\n",
    "                if ic%100==0:\n",
    "                    time.sleep(1)\n",
    "                i+=1\n",
    "                get+=1\n",
    "            except:\n",
    "                text='Error still occured in '+value[0]+value[1]+key\n",
    "                time.sleep(5+randint(1, 9)*0.1)\n",
    "                print text;logtext+=text+'\\r\\n'\n",
    "            \n",
    "    text=str(i)+' job saved after retry'\n",
    "    print text;logtext+='\\r\\n'+text+'\\r\\n'\n",
    "    print datetime.today().strftime(\"%Y/%m/%d\")+' finish: '+datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    tree=ET.ElementTree(root)\n",
    "    tree.write(open(dire_para+date_crawl+'_'+str(sect)+'.xml','wb'), encoding=\"utf-8\")\n",
    "    return logtext\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
